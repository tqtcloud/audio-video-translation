#!/usr/bin/env python3
"""
é›†æˆæµ‹è¯•è¿è¡Œå™¨
æ‰§è¡Œå®Œæ•´çš„ç³»ç»Ÿé›†æˆæµ‹è¯•å’ŒéªŒè¯
"""

import os
import sys
import time
import json
import tempfile
import threading
import asyncio
import subprocess
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import traceback
import concurrent.futures

# å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from main import AudioVideoTranslationApp
from models.core import ProcessingStage, FileType
from tests.test_data_generator import TestDataGenerator, TestDataSpec
from tests.quality_metrics import QualityAssessmentTool
from services.integrated_pipeline import PipelineConfig
from services.output_generator import OutputConfig


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹"""
    name: str
    description: str
    input_file: str
    target_language: str
    expected_duration: float
    expected_stages: List[ProcessingStage]
    quality_thresholds: Dict[str, float]
    timeout: int = 300  # 5åˆ†é’Ÿè¶…æ—¶


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_name: str
    success: bool
    processing_time: float
    stages_completed: List[ProcessingStage]
    output_file: Optional[str] = None
    quality_metrics: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    job_id: Optional[str] = None


class IntegrationTestRunner:
    """é›†æˆæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, output_dir: str = "./test_results"):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_generator = TestDataGenerator("./test_data")
        self.quality_tool = QualityAssessmentTool()
        self.app = None
        
        # æµ‹è¯•ç»Ÿè®¡
        self.test_stats = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "start_time": 0,
            "end_time": 0
        }
        
        # æµ‹è¯•ç»“æœ
        self.test_results: List[TestResult] = []
    
    def initialize_app(self) -> bool:
        """åˆå§‹åŒ–åº”ç”¨"""
        try:
            print("ğŸ”„ åˆå§‹åŒ–éŸ³é¢‘è§†é¢‘ç¿»è¯‘åº”ç”¨...")
            
            self.app = AudioVideoTranslationApp()
            
            # åˆ›å»ºæµ‹è¯•é…ç½®
            config = {
                "target_language": "zh-CN",
                "voice_model": "alloy",
                "preserve_background_audio": True,
                "output_directory": str(self.output_dir / "outputs"),
                "file_naming_pattern": "{name}_translated_{timestamp}",
                "enable_fault_tolerance": True,
                "max_retries": 2
            }
            
            # åˆå§‹åŒ–ç®¡é“
            success = self.app.initialize_pipeline(config)
            if success:
                print("âœ… åº”ç”¨åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                print("âŒ åº”ç”¨åˆå§‹åŒ–å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ åº”ç”¨åˆå§‹åŒ–å¼‚å¸¸: {str(e)}")
            return False
    
    def generate_test_cases(self) -> List[TestCase]:
        """ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        print("ğŸ“‹ ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...")
        
        test_cases = []
        
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        basic_tests = [
            TestCase(
                name="åŸºç¡€éŸ³é¢‘ç¿»è¯‘",
                description="æµ‹è¯•åŸºç¡€MP3éŸ³é¢‘æ–‡ä»¶ç¿»è¯‘åŠŸèƒ½",
                input_file="test_audio_mp3_001.mp3",
                target_language="zh-CN",
                expected_duration=10.0,
                expected_stages=[
                    ProcessingStage.FILE_VALIDATION,
                    ProcessingStage.AUDIO_EXTRACTION,
                    ProcessingStage.SPEECH_TO_TEXT,
                    ProcessingStage.TEXT_TRANSLATION,
                    ProcessingStage.TEXT_TO_SPEECH,
                    ProcessingStage.AUDIO_SYNC,
                    ProcessingStage.VIDEO_ASSEMBLY,
                    ProcessingStage.OUTPUT_GENERATION,
                    ProcessingStage.COMPLETED
                ],
                quality_thresholds={
                    "overall_quality_score": 0.7,
                    "audio_snr_db": 15.0,
                    "sync_accuracy": 0.8
                }
            ),
            TestCase(
                name="åŸºç¡€è§†é¢‘ç¿»è¯‘",
                description="æµ‹è¯•åŸºç¡€MP4è§†é¢‘æ–‡ä»¶ç¿»è¯‘åŠŸèƒ½",
                input_file="test_video_mp4_001.mp4",
                target_language="zh-CN",
                expected_duration=15.0,
                expected_stages=[
                    ProcessingStage.FILE_VALIDATION,
                    ProcessingStage.AUDIO_EXTRACTION,
                    ProcessingStage.SPEECH_TO_TEXT,
                    ProcessingStage.TEXT_TRANSLATION,
                    ProcessingStage.TEXT_TO_SPEECH,
                    ProcessingStage.AUDIO_SYNC,
                    ProcessingStage.VIDEO_ASSEMBLY,
                    ProcessingStage.OUTPUT_GENERATION,
                    ProcessingStage.COMPLETED
                ],
                quality_thresholds={
                    "overall_quality_score": 0.7,
                    "video_psnr": 25.0,
                    "sync_accuracy": 0.8
                }
            )
        ]
        
        # å¤šè¯­è¨€æµ‹è¯•
        language_tests = []
        languages = ["en", "es", "fr", "de"]
        for lang in languages:
            test_cases.append(TestCase(
                name=f"å¤šè¯­è¨€æµ‹è¯•_{lang}",
                description=f"æµ‹è¯•ç¿»è¯‘åˆ°{lang}è¯­è¨€",
                input_file="test_audio_mp3_001.mp3",
                target_language=lang,
                expected_duration=10.0,
                expected_stages=[stage for stage in ProcessingStage if stage != ProcessingStage.FAILED],
                quality_thresholds={
                    "overall_quality_score": 0.6,
                    "translation_adequacy": 0.7
                }
            ))
        
        # æ ¼å¼å…¼å®¹æ€§æµ‹è¯•
        format_tests = []
        audio_formats = ["mp3", "wav", "aac", "flac"]
        video_formats = ["mp4", "avi", "mov", "mkv"]
        
        for fmt in audio_formats:
            test_cases.append(TestCase(
                name=f"éŸ³é¢‘æ ¼å¼æµ‹è¯•_{fmt}",
                description=f"æµ‹è¯•{fmt}æ ¼å¼éŸ³é¢‘æ–‡ä»¶å¤„ç†",
                input_file=f"test_audio_{fmt}_001.{fmt}",
                target_language="zh-CN",
                expected_duration=10.0,
                expected_stages=[stage for stage in ProcessingStage if stage != ProcessingStage.FAILED],
                quality_thresholds={"overall_quality_score": 0.6}
            ))
        
        for fmt in video_formats[:2]:  # åªæµ‹è¯•å‰ä¸¤ç§è§†é¢‘æ ¼å¼
            test_cases.append(TestCase(
                name=f"è§†é¢‘æ ¼å¼æµ‹è¯•_{fmt}",
                description=f"æµ‹è¯•{fmt}æ ¼å¼è§†é¢‘æ–‡ä»¶å¤„ç†",
                input_file=f"test_video_{fmt}_001.{fmt}",
                target_language="zh-CN",
                expected_duration=15.0,
                expected_stages=[stage for stage in ProcessingStage if stage != ProcessingStage.FAILED],
                quality_thresholds={"overall_quality_score": 0.6}
            ))
        
        # è¾¹ç¼˜æƒ…å†µæµ‹è¯•
        edge_tests = [
            TestCase(
                name="çŸ­éŸ³é¢‘æ–‡ä»¶æµ‹è¯•",
                description="æµ‹è¯•æçŸ­éŸ³é¢‘æ–‡ä»¶ï¼ˆ1ç§’ï¼‰",
                input_file="test_audio_mp3_short.mp3",
                target_language="zh-CN",
                expected_duration=1.0,
                expected_stages=[stage for stage in ProcessingStage if stage != ProcessingStage.FAILED],
                quality_thresholds={"overall_quality_score": 0.5},
                timeout=120
            ),
            TestCase(
                name="é•¿éŸ³é¢‘æ–‡ä»¶æµ‹è¯•",
                description="æµ‹è¯•é•¿éŸ³é¢‘æ–‡ä»¶ï¼ˆ5åˆ†é’Ÿï¼‰",
                input_file="test_audio_wav_long.wav",
                target_language="zh-CN",
                expected_duration=300.0,
                expected_stages=[stage for stage in ProcessingStage if stage != ProcessingStage.FAILED],
                quality_thresholds={"overall_quality_score": 0.6},
                timeout=600
            )
        ]
        
        test_cases.extend(basic_tests)
        test_cases.extend(language_tests[:2])  # åªæµ‹è¯•å‰ä¸¤ç§è¯­è¨€
        test_cases.extend(format_tests[:4])    # åªæµ‹è¯•å‰å››ç§æ ¼å¼
        test_cases.extend(edge_tests)
        
        print(f"ğŸ“Š ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases
    
    def prepare_test_data(self, test_cases: List[TestCase]) -> Dict[str, str]:
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        print("ğŸ”§ å‡†å¤‡æµ‹è¯•æ•°æ®...")
        
        # æ”¶é›†éœ€è¦ç”Ÿæˆçš„æ•°æ®è§„æ ¼
        specs = []
        file_mapping = {}
        
        for test_case in test_cases:
            input_file = test_case.input_file
            
            if input_file in file_mapping:
                continue  # å·²ç»å¤„ç†è¿‡
            
            # è§£ææ–‡ä»¶åè·å–è§„æ ¼
            parts = input_file.replace('.', '_').split('_')
            if len(parts) >= 3:
                file_type = parts[1]  # audio æˆ– video
                format_name = parts[2]  # mp3, mp4 ç­‰
                
                if "short" in input_file:
                    duration = 1.0
                elif "long" in input_file:
                    duration = 300.0
                elif file_type == "audio":
                    duration = 10.0
                else:
                    duration = 15.0
                
                spec = TestDataSpec(
                    file_type=file_type,
                    format=format_name,
                    duration=duration,
                    content_type="speech",
                    language="en"
                )
                
                if file_type == "video":
                    spec.video_resolution = "1280x720"
                
                specs.append(spec)
                file_mapping[input_file] = len(specs) - 1
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        try:
            generated_files = self.data_generator.generate_test_dataset(specs)
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
            return generated_files
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {}
    
    def run_single_test(self, test_case: TestCase, test_data_files: Dict[str, str]) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_case.name}")
        print(f"ğŸ“ æè¿°: {test_case.description}")
        
        start_time = time.time()
        result = TestResult(
            test_name=test_case.name,
            success=False,
            processing_time=0.0,
            stages_completed=[]
        )
        
        try:
            # è·å–è¾“å…¥æ–‡ä»¶è·¯å¾„
            input_file_path = None
            for filename, filepath in test_data_files.items():
                if test_case.input_file in filename:
                    input_file_path = filepath
                    break
            
            if not input_file_path or not os.path.exists(input_file_path):
                result.error_message = f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_case.input_file}"
                return result
            
            print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file_path}")
            print(f"ğŸŒ ç›®æ ‡è¯­è¨€: {test_case.target_language}")
            
            # å¼€å§‹å¤„ç†
            job_id = self.app.process_file(input_file_path, test_case.target_language)
            if not job_id:
                result.error_message = "æ–‡ä»¶å¤„ç†å¯åŠ¨å¤±è´¥"
                return result
            
            result.job_id = job_id
            print(f"ğŸ†” ä½œä¸šID: {job_id}")
            
            # ç­‰å¾…å¤„ç†å®Œæˆ
            success = self.app.wait_for_completion(job_id, test_case.timeout)
            
            # è·å–æœ€ç»ˆçŠ¶æ€
            job = self.app.pipeline.get_job_status(job_id)
            if job:
                result.stages_completed = self._get_completed_stages(job.current_stage)
                
                if job.current_stage == ProcessingStage.COMPLETED:
                    result.success = True
                    result.output_file = getattr(job, 'output_file_path', None)
                    print(f"âœ… å¤„ç†æˆåŠŸå®Œæˆ")
                    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result.output_file}")
                    
                    # è¿è¡Œè´¨é‡è¯„ä¼°
                    if result.output_file and os.path.exists(result.output_file):
                        result.quality_metrics = self._assess_quality(
                            job_id, input_file_path, result.output_file, job
                        )
                        
                        # æ£€æŸ¥è´¨é‡é˜ˆå€¼
                        quality_passed = self._check_quality_thresholds(
                            result.quality_metrics, test_case.quality_thresholds
                        )
                        
                        if not quality_passed:
                            print("âš ï¸ è´¨é‡æ£€æŸ¥æœªé€šè¿‡é˜ˆå€¼è¦æ±‚")
                            result.success = False
                            result.error_message = "è´¨é‡æ£€æŸ¥æœªé€šè¿‡"
                    
                else:
                    result.error_message = getattr(job, 'error_message', 'å¤„ç†æœªå®Œæˆ')
                    print(f"âŒ å¤„ç†å¤±è´¥: {result.error_message}")
            else:
                result.error_message = "æ— æ³•è·å–ä½œä¸šçŠ¶æ€"
            
        except Exception as e:
            result.error_message = f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {str(e)}")
            traceback.print_exc()
        
        result.processing_time = time.time() - start_time
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.1f}ç§’")
        
        return result
    
    def run_concurrent_tests(self, test_cases: List[TestCase], max_workers: int = 3) -> List[TestResult]:
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        print(f"\nğŸ”„ å¼€å§‹å¹¶å‘æµ‹è¯•ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data_files = self.prepare_test_data(test_cases)
        if not test_data_files:
            print("âŒ æµ‹è¯•æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œè·³è¿‡å¹¶å‘æµ‹è¯•")
            return []
        
        results = []
        
        # åˆ†æ‰¹è¿è¡Œæµ‹è¯•ï¼ˆé¿å…èµ„æºç«äº‰ï¼‰
        batch_size = max_workers
        for i in range(0, len(test_cases), batch_size):
            batch = test_cases[i:i + batch_size]
            print(f"\nğŸ“¦ è¿è¡Œæµ‹è¯•æ‰¹æ¬¡ {i//batch_size + 1}/{(len(test_cases) + batch_size - 1)//batch_size}")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = []
                for test_case in batch:
                    future = executor.submit(self.run_single_test, test_case, test_data_files)
                    futures.append(future)
                
                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(futures):
                    try:
                        result = future.result()
                        results.append(result)
                        
                        if result.success:
                            print(f"âœ… {result.test_name}: é€šè¿‡")
                        else:
                            print(f"âŒ {result.test_name}: å¤±è´¥ - {result.error_message}")
                    
                    except Exception as e:
                        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        
        return results
    
    def run_thread_safety_tests(self) -> List[TestResult]:
        """è¿è¡Œçº¿ç¨‹å®‰å…¨æµ‹è¯•"""
        print("\nğŸ”’ å¼€å§‹çº¿ç¨‹å®‰å…¨æµ‹è¯•...")
        
        # åˆ›å»ºå¤šä¸ªç›¸åŒçš„æµ‹è¯•ä»»åŠ¡
        test_spec = TestDataSpec(
            file_type="audio",
            format="mp3",
            duration=10.0,
            content_type="speech",
            language="en"
        )
        
        # ç”Ÿæˆæµ‹è¯•æ–‡ä»¶
        test_files = self.data_generator.generate_test_dataset([test_spec] * 5)
        
        results = []
        threads = []
        
        def thread_test_worker(thread_id: int, test_file: str):
            """çº¿ç¨‹æµ‹è¯•å·¥ä½œå‡½æ•°"""
            print(f"ğŸ§µ çº¿ç¨‹ {thread_id} å¼€å§‹å¤„ç†")
            
            try:
                job_id = self.app.process_file(test_file, "zh-CN")
                if job_id:
                    success = self.app.wait_for_completion(job_id, 300)
                    job = self.app.pipeline.get_job_status(job_id)
                    
                    result = TestResult(
                        test_name=f"çº¿ç¨‹å®‰å…¨æµ‹è¯•_{thread_id}",
                        success=success and job and job.current_stage == ProcessingStage.COMPLETED,
                        processing_time=getattr(job, 'processing_time', 0) if job else 0,
                        stages_completed=self._get_completed_stages(job.current_stage) if job else [],
                        job_id=job_id,
                        output_file=getattr(job, 'output_file_path', None) if job else None
                    )
                else:
                    result = TestResult(
                        test_name=f"çº¿ç¨‹å®‰å…¨æµ‹è¯•_{thread_id}",
                        success=False,
                        processing_time=0,
                        stages_completed=[],
                        error_message="ä½œä¸šåˆ›å»ºå¤±è´¥"
                    )
                
                results.append(result)
                print(f"ğŸ§µ çº¿ç¨‹ {thread_id} å®Œæˆ: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
                
            except Exception as e:
                result = TestResult(
                    test_name=f"çº¿ç¨‹å®‰å…¨æµ‹è¯•_{thread_id}",
                    success=False,
                    processing_time=0,
                    stages_completed=[],
                    error_message=f"çº¿ç¨‹å¼‚å¸¸: {str(e)}"
                )
                results.append(result)
                print(f"ğŸ§µ çº¿ç¨‹ {thread_id} å¼‚å¸¸: {str(e)}")
        
        # å¯åŠ¨å¤šä¸ªçº¿ç¨‹
        file_list = list(test_files.values())
        for i in range(min(3, len(file_list))):  # æœ€å¤š3ä¸ªå¹¶å‘çº¿ç¨‹
            thread = threading.Thread(
                target=thread_test_worker,
                args=(i + 1, file_list[i]),
                name=f"ThreadSafetyTest-{i+1}"
            )
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join(timeout=400)  # æ¯ä¸ªçº¿ç¨‹æœ€å¤šç­‰å¾…400ç§’
        
        print(f"ğŸ”’ çº¿ç¨‹å®‰å…¨æµ‹è¯•å®Œæˆï¼Œè¿è¡Œäº† {len(results)} ä¸ªå¹¶å‘ä»»åŠ¡")
        return results
    
    def run_performance_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nâš¡ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        benchmarks = {}
        
        # ç”Ÿæˆä¸åŒå¤§å°çš„æµ‹è¯•æ–‡ä»¶
        test_specs = [
            TestDataSpec(file_type="audio", format="mp3", duration=10.0, content_type="speech"),   # å°æ–‡ä»¶
            TestDataSpec(file_type="audio", format="mp3", duration=60.0, content_type="speech"),   # ä¸­æ–‡ä»¶
            TestDataSpec(file_type="audio", format="mp3", duration=300.0, content_type="speech"),  # å¤§æ–‡ä»¶
            TestDataSpec(file_type="video", format="mp4", duration=30.0, content_type="speech", video_resolution="1280x720"),  # è§†é¢‘æ–‡ä»¶
        ]
        
        test_files = self.data_generator.generate_test_dataset(test_specs)
        
        for i, (filename, filepath) in enumerate(test_files.items()):
            file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
            print(f"ğŸ“Š åŸºå‡†æµ‹è¯• {i+1}: {filename} ({file_size:.1f}MB)")
            
            start_time = time.time()
            start_memory = self._get_memory_usage()
            
            try:
                job_id = self.app.process_file(filepath, "zh-CN")
                if job_id:
                    success = self.app.wait_for_completion(job_id, 600)  # 10åˆ†é’Ÿè¶…æ—¶
                    end_time = time.time()
                    end_memory = self._get_memory_usage()
                    
                    processing_time = end_time - start_time
                    memory_usage = end_memory - start_memory
                    
                    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                    throughput = file_size / processing_time if processing_time > 0 else 0  # MB/s
                    
                    benchmarks[f"benchmark_{i+1}"] = {
                        "filename": filename,
                        "file_size_mb": file_size,
                        "processing_time_seconds": processing_time,
                        "memory_usage_mb": memory_usage,
                        "throughput_mb_per_second": throughput,
                        "success": success
                    }
                    
                    print(f"  â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")
                    print(f"  ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_usage:.1f}MB")
                    print(f"  ğŸš€ ååé‡: {throughput:.2f}MB/s")
                
            except Exception as e:
                print(f"  âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
                benchmarks[f"benchmark_{i+1}"] = {
                    "filename": filename,
                    "error": str(e)
                }
        
        return benchmarks
    
    def run_error_recovery_tests(self) -> List[TestResult]:
        """è¿è¡Œé”™è¯¯æ¢å¤æµ‹è¯•"""
        print("\nğŸ› ï¸ å¼€å§‹é”™è¯¯æ¢å¤æµ‹è¯•...")
        
        results = []
        
        # æµ‹è¯•åœºæ™¯
        error_scenarios = [
            {
                "name": "ä¸å­˜åœ¨çš„æ–‡ä»¶",
                "input_file": "/nonexistent/file.mp3",
                "expected_error": "æ–‡ä»¶ä¸å­˜åœ¨"
            },
            {
                "name": "æŸåçš„æ–‡ä»¶",
                "input_file": self._create_corrupted_file(),
                "expected_error": "æ–‡ä»¶éªŒè¯å¤±è´¥"
            },
            {
                "name": "ä¸æ”¯æŒçš„æ ¼å¼",
                "input_file": self._create_unsupported_file(),
                "expected_error": "ä¸æ”¯æŒçš„æ ¼å¼"
            }
        ]
        
        for scenario in error_scenarios:
            print(f"ğŸ§ª æµ‹è¯•é”™è¯¯åœºæ™¯: {scenario['name']}")
            
            start_time = time.time()
            
            try:
                job_id = self.app.process_file(scenario['input_file'], "zh-CN")
                
                if job_id:
                    # ç­‰å¾…å¤„ç†å®Œæˆæˆ–å¤±è´¥
                    self.app.wait_for_completion(job_id, 60)  # 1åˆ†é’Ÿè¶…æ—¶
                    job = self.app.pipeline.get_job_status(job_id)
                    
                    if job and job.current_stage == ProcessingStage.FAILED:
                        result = TestResult(
                            test_name=f"é”™è¯¯æ¢å¤_{scenario['name']}",
                            success=True,  # é¢„æœŸå¤±è´¥ï¼Œæ‰€ä»¥æˆåŠŸå¤„ç†é”™è¯¯ç®—ä½œæµ‹è¯•é€šè¿‡
                            processing_time=time.time() - start_time,
                            stages_completed=[ProcessingStage.FAILED],
                            error_message=f"æŒ‰é¢„æœŸå¤±è´¥: {getattr(job, 'error_message', 'æœªçŸ¥é”™è¯¯')}"
                        )
                        print(f"  âœ… é”™è¯¯æ­£ç¡®å¤„ç†: {result.error_message}")
                    else:
                        result = TestResult(
                            test_name=f"é”™è¯¯æ¢å¤_{scenario['name']}",
                            success=False,
                            processing_time=time.time() - start_time,
                            stages_completed=[],
                            error_message="æœªèƒ½æ­£ç¡®å¤„ç†é¢„æœŸé”™è¯¯"
                        )
                        print(f"  âŒ é”™è¯¯å¤„ç†ä¸å½“")
                else:
                    # ç«‹å³å¤±è´¥ä¹Ÿæ˜¯æ­£ç¡®çš„é”™è¯¯å¤„ç†
                    result = TestResult(
                        test_name=f"é”™è¯¯æ¢å¤_{scenario['name']}",
                        success=True,
                        processing_time=time.time() - start_time,
                        stages_completed=[],
                        error_message="ç«‹å³è¯†åˆ«å¹¶æ‹’ç»æ— æ•ˆè¾“å…¥"
                    )
                    print(f"  âœ… ç«‹å³è¯†åˆ«é”™è¯¯")
                
            except Exception as e:
                # å¼‚å¸¸ä¹Ÿå¯èƒ½æ˜¯æ­£ç¡®çš„é”™è¯¯å¤„ç†æ–¹å¼
                result = TestResult(
                    test_name=f"é”™è¯¯æ¢å¤_{scenario['name']}",
                    success=True,
                    processing_time=time.time() - start_time,
                    stages_completed=[],
                    error_message=f"é€šè¿‡å¼‚å¸¸å¤„ç†é”™è¯¯: {str(e)}"
                )
                print(f"  âœ… é€šè¿‡å¼‚å¸¸å¤„ç†: {str(e)}")
            
            results.append(result)
        
        return results
    
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¼€å§‹ç»¼åˆæµ‹è¯•å¥—ä»¶...")
        
        self.test_stats["start_time"] = time.time()
        
        # åˆå§‹åŒ–åº”ç”¨
        if not self.initialize_app():
            return {"error": "åº”ç”¨åˆå§‹åŒ–å¤±è´¥"}
        
        comprehensive_results = {
            "test_summary": {},
            "functional_tests": [],
            "concurrent_tests": [],
            "thread_safety_tests": [],
            "performance_benchmarks": {},
            "error_recovery_tests": [],
            "overall_statistics": {}
        }
        
        try:
            # 1. åŠŸèƒ½æµ‹è¯•
            print("\n" + "="*50)
            print("ğŸ“‹ ç¬¬ä¸€é˜¶æ®µï¼šåŠŸèƒ½æµ‹è¯•")
            print("="*50)
            
            test_cases = self.generate_test_cases()
            self.test_stats["total_tests"] += len(test_cases)
            
            test_data_files = self.prepare_test_data(test_cases)
            
            for test_case in test_cases[:5]:  # åªè¿è¡Œå‰5ä¸ªæ ¸å¿ƒæµ‹è¯•ç”¨ä¾‹
                result = self.run_single_test(test_case, test_data_files)
                comprehensive_results["functional_tests"].append(asdict(result))
                self.test_results.append(result)
                
                if result.success:
                    self.test_stats["passed_tests"] += 1
                else:
                    self.test_stats["failed_tests"] += 1
            
            # 2. å¹¶å‘æµ‹è¯•
            print("\n" + "="*50)
            print("ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘æµ‹è¯•")
            print("="*50)
            
            concurrent_test_cases = test_cases[:3]  # é€‰æ‹©3ä¸ªæµ‹è¯•ç”¨ä¾‹è¿›è¡Œå¹¶å‘æµ‹è¯•
            concurrent_results = self.run_concurrent_tests(concurrent_test_cases, max_workers=2)
            
            for result in concurrent_results:
                comprehensive_results["concurrent_tests"].append(asdict(result))
                self.test_results.append(result)
                self.test_stats["total_tests"] += 1
                
                if result.success:
                    self.test_stats["passed_tests"] += 1
                else:
                    self.test_stats["failed_tests"] += 1
            
            # 3. çº¿ç¨‹å®‰å…¨æµ‹è¯•
            print("\n" + "="*50)
            print("ğŸ”’ ç¬¬ä¸‰é˜¶æ®µï¼šçº¿ç¨‹å®‰å…¨æµ‹è¯•")
            print("="*50)
            
            thread_safety_results = self.run_thread_safety_tests()
            for result in thread_safety_results:
                comprehensive_results["thread_safety_tests"].append(asdict(result))
                self.test_results.append(result)
                self.test_stats["total_tests"] += 1
                
                if result.success:
                    self.test_stats["passed_tests"] += 1
                else:
                    self.test_stats["failed_tests"] += 1
            
            # 4. æ€§èƒ½åŸºå‡†æµ‹è¯•
            print("\n" + "="*50)
            print("âš¡ ç¬¬å››é˜¶æ®µï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•")
            print("="*50)
            
            benchmark_results = self.run_performance_benchmarks()
            comprehensive_results["performance_benchmarks"] = benchmark_results
            
            # 5. é”™è¯¯æ¢å¤æµ‹è¯•
            print("\n" + "="*50)
            print("ğŸ› ï¸ ç¬¬äº”é˜¶æ®µï¼šé”™è¯¯æ¢å¤æµ‹è¯•")
            print("="*50)
            
            error_recovery_results = self.run_error_recovery_tests()
            for result in error_recovery_results:
                comprehensive_results["error_recovery_tests"].append(asdict(result))
                self.test_results.append(result)
                self.test_stats["total_tests"] += 1
                
                if result.success:
                    self.test_stats["passed_tests"] += 1
                else:
                    self.test_stats["failed_tests"] += 1
            
        except Exception as e:
            print(f"âŒ ç»¼åˆæµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            traceback.print_exc()
            comprehensive_results["error"] = str(e)
        
        finally:
            self.test_stats["end_time"] = time.time()
            
            # å…³é—­åº”ç”¨
            if self.app:
                self.app.shutdown()
        
        # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡
        comprehensive_results["overall_statistics"] = self._generate_final_statistics()
        
        # ä¿å­˜ç»“æœ
        self._save_test_results(comprehensive_results)
        
        return comprehensive_results
    
    def _get_completed_stages(self, current_stage: ProcessingStage) -> List[ProcessingStage]:
        """è·å–å·²å®Œæˆçš„é˜¶æ®µ"""
        all_stages = [
            ProcessingStage.FILE_VALIDATION,
            ProcessingStage.AUDIO_EXTRACTION,
            ProcessingStage.SPEECH_TO_TEXT,
            ProcessingStage.TEXT_TRANSLATION,
            ProcessingStage.TEXT_TO_SPEECH,
            ProcessingStage.AUDIO_SYNC,
            ProcessingStage.VIDEO_ASSEMBLY,
            ProcessingStage.OUTPUT_GENERATION,
            ProcessingStage.COMPLETED
        ]
        
        completed = []
        for stage in all_stages:
            if stage.value <= current_stage.value:
                completed.append(stage)
            else:
                break
        
        return completed
    
    def _assess_quality(self, job_id: str, input_file: str, output_file: str, job) -> Dict[str, Any]:
        """è¯„ä¼°å¤„ç†è´¨é‡"""
        try:
            # æ„å»ºå¤„ç†ç»“æœå­—å…¸
            processing_results = getattr(job, 'intermediate_results', {})
            
            # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            quality_report = self.quality_tool.generate_quality_report(
                job_id, input_file, output_file, processing_results
            )
            
            return quality_report
            
        except Exception as e:
            print(f"âš ï¸ è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _check_quality_thresholds(self, quality_metrics: Dict[str, Any], thresholds: Dict[str, float]) -> bool:
        """æ£€æŸ¥è´¨é‡é˜ˆå€¼"""
        if not quality_metrics or "metrics" not in quality_metrics:
            return False
        
        for threshold_name, threshold_value in thresholds.items():
            actual_value = self._extract_metric_value(quality_metrics, threshold_name)
            
            if actual_value is None or actual_value < threshold_value:
                print(f"  âš ï¸ è´¨é‡æŒ‡æ ‡ {threshold_name} ä¸è¾¾æ ‡: {actual_value} < {threshold_value}")
                return False
        
        return True
    
    def _extract_metric_value(self, quality_metrics: Dict[str, Any], metric_name: str) -> Optional[float]:
        """ä»è´¨é‡æŒ‡æ ‡ä¸­æå–ç‰¹å®šå€¼"""
        metrics = quality_metrics.get("metrics", {})
        
        if metric_name == "overall_quality_score":
            return quality_metrics.get("overall_quality_score")
        elif metric_name == "audio_snr_db":
            audio_metrics = metrics.get("audio_quality", {})
            return audio_metrics.get("snr_db")
        elif metric_name == "video_psnr":
            video_metrics = metrics.get("video_quality", {})
            return video_metrics.get("psnr")
        elif metric_name == "sync_accuracy":
            sync_metrics = metrics.get("sync_quality", {})
            return sync_metrics.get("timing_accuracy")
        elif metric_name == "translation_adequacy":
            trans_metrics = metrics.get("translation_quality", {})
            return trans_metrics.get("adequacy_score")
        
        return None
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / (1024 * 1024)
        except:
            return 0.0
    
    def _create_corrupted_file(self) -> str:
        """åˆ›å»ºæŸåçš„æµ‹è¯•æ–‡ä»¶"""
        corrupted_file = tempfile.NamedTemporaryFile(suffix=".mp3", delete=False)
        corrupted_file.write(b"This is not a valid MP3 file" * 100)
        corrupted_file.close()
        return corrupted_file.name
    
    def _create_unsupported_file(self) -> str:
        """åˆ›å»ºä¸æ”¯æŒæ ¼å¼çš„æµ‹è¯•æ–‡ä»¶"""
        unsupported_file = tempfile.NamedTemporaryFile(suffix=".xyz", delete=False)
        unsupported_file.write(b"Unsupported file format")
        unsupported_file.close()
        return unsupported_file.name
    
    def _generate_final_statistics(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        total_time = self.test_stats["end_time"] - self.test_stats["start_time"]
        success_rate = (self.test_stats["passed_tests"] / self.test_stats["total_tests"] * 100) if self.test_stats["total_tests"] > 0 else 0
        
        statistics = {
            "æ€»æµ‹è¯•æ•°é‡": self.test_stats["total_tests"],
            "é€šè¿‡æµ‹è¯•æ•°é‡": self.test_stats["passed_tests"],
            "å¤±è´¥æµ‹è¯•æ•°é‡": self.test_stats["failed_tests"],
            "æˆåŠŸç‡": f"{success_rate:.1f}%",
            "æ€»æµ‹è¯•æ—¶é—´": f"{total_time:.1f}ç§’",
            "å¹³å‡æµ‹è¯•æ—¶é—´": f"{total_time / max(self.test_stats['total_tests'], 1):.1f}ç§’"
        }
        
        # åˆ†æå¸¸è§å¤±è´¥åŸå› 
        failure_reasons = {}
        for result in self.test_results:
            if not result.success and result.error_message:
                reason = result.error_message.split(':')[0]  # å–é”™è¯¯æ¶ˆæ¯çš„ç¬¬ä¸€éƒ¨åˆ†
                failure_reasons[reason] = failure_reasons.get(reason, 0) + 1
        
        if failure_reasons:
            statistics["ä¸»è¦å¤±è´¥åŸå› "] = failure_reasons
        
        return statistics
    
    def _save_test_results(self, results: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.output_dir / f"comprehensive_test_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
        summary_file = self.output_dir / f"test_summary_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("éŸ³é¢‘è§†é¢‘ç¿»è¯‘ç³»ç»Ÿ - ç»¼åˆæµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            stats = results.get("overall_statistics", {})
            for key, value in stats.items():
                f.write(f"{key}: {value}\n")
            
            f.write("\nè¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: " + str(results_file.name) + "\n")
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜:")
        print(f"  ğŸ“„ è¯¦ç»†ç»“æœ: {results_file}")
        print(f"  ğŸ“„ æ‘˜è¦æŠ¥å‘Š: {summary_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ éŸ³é¢‘è§†é¢‘ç¿»è¯‘ç³»ç»Ÿ - ç»¼åˆæµ‹è¯•å’Œè´¨é‡éªŒè¯")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = IntegrationTestRunner()
    
    try:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        results = runner.run_comprehensive_tests()
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»Ÿè®¡")
        print("=" * 60)
        
        stats = results.get("overall_statistics", {})
        for key, value in stats.items():
            print(f"{key}: {value}")
        
        # åˆ¤æ–­æ•´ä½“æµ‹è¯•ç»“æœ
        if "æˆåŠŸç‡" in stats:
            success_rate = float(stats["æˆåŠŸç‡"].replace("%", ""))
            if success_rate >= 80:
                print("\nğŸ‰ ç»¼åˆæµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè´¨é‡è‰¯å¥½ã€‚")
                return 0
            elif success_rate >= 60:
                print("\nâš ï¸ ç»¼åˆæµ‹è¯•éƒ¨åˆ†é€šè¿‡ï¼Œç³»ç»Ÿå­˜åœ¨ä¸€äº›é—®é¢˜éœ€è¦æ”¹è¿›ã€‚")
                return 1
            else:
                print("\nâŒ ç»¼åˆæµ‹è¯•å¤±è´¥ï¼Œç³»ç»Ÿå­˜åœ¨ä¸¥é‡é—®é¢˜ã€‚")
                return 2
        else:
            print("\nâŒ æ— æ³•è·å–æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ã€‚")
            return 3
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())